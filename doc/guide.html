<!DOCTYPE html >
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Aluesarjat: Simplified Operation & Maintenance Guide</title>
<style type="text/css">
    dt {
        font-family: "courier";
    }
    h1 {
        font-size: 1.5em;
    }
    h2 {
        font-size: 1.4em;
    }
    h3 {
        font-size: 1.2em;
    }    
</style>
</head>
<body>
<h1>Aluesarjat: Simplified Operation & Maintenance Guide</h1>

<h2>Contents</h2>

<ul>
    <li><a href="#sysreq">System requirements</a></li>
    <li><a href="#dev">Development</a></li>
    <li><a href="#pcaxis">PCAxis parser</a></li>
    <li><a href="#rdfstorage">RDF storage</a></li>
    <li><a href="#urischema">URI Schema</a></li>
    <li><a href="#services">Services</a></li>
    <li><a href="#webapges">Web pages</a></li>
    <li><a href="#linkeddata">Linked data</a></li>
    <li><a href="#sparql">SPARQL query examples</a></li>
</ul>


<h2 id="sysreq">System requirements</h2>

Versions numbers refer to ones used in development and testing. Newer versions and
any Java compliant platform/JVM should however suffice - at least with some tweaking.
<ul>
    <li>Linux Ubuntu, Mac OS X 10.6</li>
    <li>Java SE 1.6</li>
    <li>Maven 2.2</li>
    <li>OpenLink Virtuoso Universal Server (with RDF/SPARQL support) 6.1</li>
</ul>

<h2 id="dev">Development</h2>

<h3>Tools</h3>
<ul>
<li>Eclipse</li>
<li>SVN</li>
<li>SVN plugin for Eclipse</li>
<li>Virtuoso <i>is not</i> needed for basic development and testing, which is done using
Sesame Native RDF repository. It requires no additional configuration and stores it's data
in <tt>${java.io.tmpdir}/aluesarjat/data</tt>. Improper shutdown of webapp may corrupt this database.
However all data can always be securely reloaded by deleting the database and restarting the app.</li>
</ul>

<h3>Project structure</h3>
<dl>
<dt>src/test/java</dt>
<dd>Unit Tests (JUnit) and helpper applications (e.g. webapp starter and various to-RDF-conversion applications)</dd>

<dt>src/test/resources</dt>
<dd>Files needed by tests and original export files</dd>

<dt>src/main/java</dt>
<dd>Java classes of the main application: PCAxis parser, PCAxis-to-RDF converter and Webapp</dd>

<dt>src/main/resources</dt>
<dd>Webapp configuration and default data files</dd>

<dt>doc</dt>
<dd>Documents</dd>

<dt>profiles</dt>
<dd>Deployment profiles</dd>

<dt>target</dt>
<dd>Internal build work directory. Not to be modified by hand.</dd>
</dl>

<h3>Getting Started</h3>
<ol>
<li>Import aluesarjat into Eclipse as general purpose project.
</li>
<li>
Open console and go to project directory.
</li>
<li>Prepare and verify development setup by runing 
<pre>
mvn clean eclipse:eclipse -DdownloadSources=true install
</pre>
</li>
<li>
Refresh and clean project in Eclipse.
</li>
</ol>

<h3>Testing and Running Webapp on localhost</h3>
<ul>
<li>Tests can be run individually in Eclipse using JUnit test runner</li>
<li>All tests are run as part of Maven build life cycle (e.g. <tt>mvn test</tt> or <tt>mvn install</tt>)</li>
<li>Webapp can be run using either 
<dl>
<dt>AluesarjatStart</dt><dd>With build-in Sesame native RDF repository</dd>
<dt>AluesarjatVirtuosoStart</dt><dd>With high scalability Virtuoso RDF repository. 
Virtuoso's connection properties are read from the classpath resource <tt>aluesarjat.properties: virtuoso.host,
virtuoso.port,
virtuoso.user, </tt>and 
<tt>virtuoso.pass.</tt> 
</dd>
</dl>
</li>
</ul>


<h2 id="pcaxis">PCAxis parser</h2>

PCAxisParser is a streaming parser for PX files. It converts PX files into a minimal statistical class 
model (see below) that consists of Dataset, DimensionType, Dimension and Item. 
First it collects all relevant 
metadata into a Dataset instance which is given to the handler. Then it streams all DATA items 
as Item instances to the handler. The handler has also methods for begin, rollback and commit, for 
proper allocation and releasing of required resources (e.g. DB connection or transaction).

<img src="parser.png" alt="PCAxis parser"/>

<p>RDFDatasetHandler converts Datasets and Items
into Scovo based RDF that is further streamed into database in optimized batches. 
ANTLR based parser was first tested, but it proved
unscalable for larger PX files - even heap size of 2GB was not enough to hold the intermediate AST tree. 
</p>

<h2 id="rdfstorage">RDF storage</h2>

<p><a href="http://virtuoso.openlinksw.com/">OpenLink Virtuoso</a> is the primary RDF storage system of this project. 
As the embedded Sesame NativeStore didn't scale anymore when
lots of datasets where added, both BigData and OpenLink Virtuoso where evaluated as scalable alternatives.</p>

<p>The configuration and usage of BigData proved to be too complicated for this project and Virtuoso was chosen.</p>

<p>Virtuoso is used via an external process running the database and the Java application uses it via a dedicated RDFBean module, which
was developed in the course of this project.</p>

<h2 id="urischema">URI Schema</h2>

With RDF one URI schema is needed for resource identifiers. Another URI schema is needed for Web Application.
In this Linked Open Data pilot we combines these as much as possible following REST principles. 
Thus where ever practical RDF data is accessbile using real URI's.
All URIs share common base URI which is the URI of the application it self.
BaseURI is configured in <tt>aluesarjat.properties: baseURI</tt>.

<h3>Data (RDF Resource) URIs</h3>

These URIs by which given type of data is exposed as RDF are also used internally as contexts for
corresponding RDF statements. 

<p>While the RDF data model consists of subject-predicate-object 
triples, in many practical applications one handles quadruples instead under hood with source (or context or model)
as a fourth member. This context may be any URI but usually it is the main namespace of the triples without 
namespace/local name delimiter (e.g. trailing # or /).</p>

<dl>
<dt>${baseURI}/rdf/dimensions</dt>
<dd>Common dimension metamodel: All scovo:Dimension (sub) classes - i.e. dimension types. 
Namespace / local name delimiter for dimension types is "/". E.g. dimensions/Alue</dd>

<dt>${baseURI}/rdf/dimensions/${dimension}</dt>
<dd>Values of given dimension. Namespace / local name delimiter for dimension values is "#". E.g. dimension/Alue#_091_Helsinki</dd>

<dt>${baseURI}/rdf/datasets</dt>
<dd>Dataset metadata. Namespace / local name delimiter for dataset URIs' is "#". E.g. datasets#A01HKI_Astuot_hper_rahoitus_talotyyppi. 
Data items of individual datasets are internally mapped to a context denoted by dataset URI. 
As this dataset URI is partly composed of fragment ID, items are not directly accessible as RDF.
This limitation is by design, because the size of RDF serialization of items of even 
a single dataset may be several Giga Bytes.</dd>

<dt>item:${sha1 of item predicates} - INTERNAL</dt>
<dd>As publishing individual items directly in either dataset wide RDF bundles nor individuals
is not practical, Blank Node would be a natural choice for item identifiers. 
Unfortunately there are severe limitations to querying and modifying RDF data with BNodes: BNodes cannot 
be accessed using standard SPARQL and typically cannot be referenced from outside a database transaction.
What's more, BNode behaviour differs from RDF database implementation to another.
<p>As a practical approach we chose a proprietary URI scheme for data item ID's. This ID consists
of "item:" prefix and a SHA-1 checksum of item's statements. This way we may for example reload
data without duplicates - as long as the algorithm or data it self havent changed.
These items are also accessible using SPARQL, without which building the webapp would have been 
conciderably more difficult. 
</dd>
</dl>

<h3>Webapp URIs and Servlets</h3>

Webapp consists of Servlets that are mapped to URLs using Guice Servlet extension. Guice is 
also used for wiring application dependencies (dependency injection).
</body>

<h2 id="services">Services</h2>

<p>The Web pages interact with the backend through a number of services, which are presented below.</p>

<h3>SPARQL endpoint</h3>

<p>The SPARQL endpoint is available via /sparql and acts according to the following <a href="http://www.w3.org/TR/rdf-sparql-protocol/">SPARQL protocol</a> specification  </p>

<h3>Faceted search</h3>

<p>The facected search functionality is provided via /search and /facets. The /facets service provides data on all available dimension values
in JSON form via HTTP GET /facets.</p>

<p>The /search service provides the actual search functionality via HTTP GET /search. Via "value" named request parameters the result is constrained
to items having the given dimension values, e.g. value=valmistumisvuosi:_1940-1949 to search for items having the given dimension value.</p>

<p>The parameter "include" is used to declare if "items" and/or "facets" are to be returned. Use include=items and include=facets to return both.</p>

<p>"limit" and "offset" can be used for paging of the results. The semantics are the same as in SQL. </p>

<p>The result is an object with the optional entries "items" for matched items, "facets" for additional dimension values to be chosen and optinally
"hasMoreResults" with the value true, if more results are available.</p>

<h2 id="webpages">Web pages</h2>

<h3>Faceted search</h3>

<p>The Faceted search displays a list of all dimension types and dimensions loaded into the service. Dimension values can be selected and unselected
by clicking on them. Each selection reduces the list of shown dimensions to those used in items having the selected dimensions.</p>

<p>A minimum of three dimension selections is necessary to show items having the selected dimesions on the right hand side of the window.
The dimension buttons on the right hand side can be used as well to add and remove dimension selections.</p>

<p>The header of the item results shows the selected dimensions and the body shows the item data in tabular form.</p>

<p>The page size of results can be controlled via the Page size field. For cases where the result size exceeds the page size, the Previous and Next page
links can be used for navigation between the result pages.</p>

<h3>SPARQL query form</h3>

<p>The SPARQL query form provides an easy to use interface to execute SPARQL queries against the integrated HTTP based SPARQL endpoint.
The queries are entered into the query field and either executed via the Submit Query -button or saved into the list of saved queries via the
Save Query -button. Default namespaces for most of the query cases are by default used and needn't be declared in the Query field.</p>

<p>The page size of results can be controlled via the Page size field. For cases where the result size exceeds the page size, the Previous and Next page
links can be used for navigation between the result pages.</p>

<p>Saved queries can be copied into the Query form by single clicks and deleted from the list of saved queries by double clicks.</p>

<h3>Map visualization</h3>

<p>The Map visualization uses a GoogleMaps widget to show the area borders of the Capital Region of Helsinki. The visualization
uses three layers of widgets to show large areas for small zoom levels, medium areas for medium zoom range and small areas for high zoom range.
</p>

<p>When crossing an area with the mouse, the background opacity of the area becomes smaller and a marker with the area title is shown at the center
point of the area. When clicking an area related data is shown in the info widget on the right of the window. The tabs of the info widget contain
a general description of the area, the DBpedia description of the area and a small excerpt of the area statistics. </p>

<p></p>

<h2 id="linkeddata">Linked data</h2>

<p>The application contains linked data from DBPedia and Tarinoiden Helsinki.</p>

<p>The linking has been done by creating link triples between Area resources of the statistical data and resources of the external datasets.</p>

<p>Here is an example of link data between the local areas and DBpedia resources. The owl:sameAs relation is used to state that the resources are 
equivalent :</p>

<pre>
@prefix alue: &lt;http://localhost:8080/rdf/dimensions/Alue#&gt; .
@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .
@prefix dbpedia: &lt;http://dbpedia.org/resource/&gt; .

alue:_091_70_Ullanlinna owl:sameAs dbpedia:Ullanlinna .
alue:_091_240_Kumpula owl:sameAs dbpedia:Kumpula .
alue:_091_282_Maunula owl:sameAs dbpedia:Maunula .
alue:_091_383_Pihlajamäki owl:sameAs &lt;http://dbpedia.org/resource/Pihlajam%C3%A4ki&gt; .
alue:_091_495_Hevossalmi owl:sameAs dbpedia:Hevossalmi .
alue:_092_34_Kiila owl:sameAs dbpedia:Kiila .
alue:_091_111_Siltasaari owl:sameAs dbpedia:Siltasaari .
alue:_091_210_Hermanni owl:sameAs dbpedia:Hermanni .
</pre>

<p>To get the local areas mapped to the descriptions of the DBPedia data, the following query can be used :</p>

<pre>
SELECT ?area ?comment 
WHERE { 
    ?area rdf:type dimension:Alue ; owl:sameAs ?area2 . 
    ?area2 rdfs:comment ?comment 
}
</pre>

<p>And here is an example of the linkage data between the local areas and Tarinoiden Helsinki :</p>

<pre>
@prefix alue: &lt;http://localhost:8080/rdf/dimensions/Alue#&gt; .
@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .

alue:_091_702_Myllypuron_peruspiiri owl:sameAs &lt;http://www.tarinoidenhelsinki.fi/resource/place/97/&gt; .
alue:_091_130_Etu-Töölö owl:sameAs &lt;http://www.tarinoidenhelsinki.fi/resource/place/24/&gt; .
alue:_091_60_Eira owl:sameAs &lt;http://www.tarinoidenhelsinki.fi/resource/place/36/&gt; .
alue:_091_333_Malminkartano owl:sameAs &lt;http://www.tarinoidenhelsinki.fi/resource/place/2/&gt; .
alue:_091_474_Kivikko owl:sameAs &lt;http://www.tarinoidenhelsinki.fi/resource/place/99/&gt; .
alue:_091_402_Länsi-Pakilan_peruspiiri owl:sameAs &lt;http://www.tarinoidenhelsinki.fi/resource/place/62/&gt; .
alue:_091_231_Toukola owl:sameAs &lt;http://www.tarinoidenhelsinki.fi/resource/place/53/&gt; .
alue:_091_293_Pohjois-Haaga owl:sameAs &lt;http://www.tarinoidenhelsinki.fi/resource/place/7/&gt; .
alue:_091_391_Tapaninvainio owl:sameAs &lt;http://www.tarinoidenhelsinki.fi/resource/place/73/&gt; .
</pre>

<p>The same approach of creating links via owl:sameAs can be applied also to other related datasets to get the benefits of Linked data.</p>

<h2 id="sparql">SPARQL query examples</h2>

<h3>Dimension Types</h3>
<pre>
SELECT ?dimensionName ?dimensionURI 
WHERE { 
    ?dimensionURI rdfs:subClassOf scv:Dimension ; 
    dc:title ?dimensionName . 
}
</pre>

<h3>Dimensions</h3>
<pre>
SELECT ?dimension ?dimensionValue ?dimensionValueName
WHERE { 
    ?dimension rdfs:subClassOf scv:Dimension .
    ?dimensionValue rdf:type ?dimension ;
    dc:title ?dimensionValueName .
}
</pre>

<h3>Areas</h3>
<pre>
SELECT ?areaName ?areaURI 
WHERE { 
    ?areaURI rdf:type dimension:Alue ;
    dc:title ?areaName . 
}
</pre>

<h3>Datasets</h3>
<pre>
SELECT ?dataset
WHERE { 
    ?d rdf:type scv:Dataset ; dc:title ?dataset .
}
</pre>

<h3>Datasets with dimension types</h3>
<pre>
SELECT distinct ?datasetName ?dataset ?dimensionType
WHERE { 
    ?dataset rdf:type scv:Dataset ; 
    dc:title ?datasetName ;
    stat:datasetDimension [ rdf:type ?dimensionType ] .
}
order by ?datasetName
</pre>

<h3>Datasets with timestamps</h3>
<pre>
SELECT ?name ?dataset ?created ?modified
WHERE { 
    ?dataset rdf:type scv:Dataset ; 
    dc:title ?name ; 
    dcterms:created ?created .
    OPTIONAL { ?dataset dcterms:modified ?modified . }
}
</pre>

<h3>Statistics of Kallio in 2000</h3>
<pre>
SELECT ?dataset ?item ?value ?dimension
WHERE {
  ?item dimension:alue alue:_091_301_Kallion_peruspiiri ;
    dimension:vuosi vuosi:_2009 ;
    scv:dataset ?dataset ;
    ?dimensionProperty ?dimension ;
    rdf:value ?value .
  ?dimensionProperty rdfs:subPropertyOf scv:dimension .
}
ORDER BY ?dataset
</pre>

<h3>Alueet, joilla enemmän 0-6 vuotiaita kuin 7-12 vuotiaita vuonna 2009</h3>
<pre>
SELECT DISTINCT ?area ?young ?old
WHERE { 
  {
    [ dimension:ikäryhmä ikäryhmä:_0-6-vuotiaat ;
      dimension:vuosi vuosi:_2009 ;
      scv:dataset ?dataset ; 
      dimension:alue ?area ;
      rdf:value ?young ].
    OPTIONAL {
      [ dimension:ikäryhmä ikäryhmä:_7-12-vuotiaat ;
        dimension:vuosi vuosi:_2009 ;
        scv:dataset ?dataset ; 
        dimension:alue ?area ;
        rdf:value ?old ] .
    }
  }
  FILTER ( ?young > ?old ).
} 


</pre>

<h3>Taulukko 1. Väkiluku ikäryhmittäin 1. tammikuuta ja ennuste</h3>
<pre>
SELECT ?ik ?v ?val 
WHERE { 
 ?i dimension:vuosi ?v .  
 FILTER ( ?v = vuosi:_2000 || ?v = vuosi:_2008 || ?v = vuosi:_2009 || ?v = vuosi:_2010 ) 
 ?i dimension:yksikkö yksikkö:Henkilöä . 
 ?i dimension:ikäryhmä ?ik . 
 ?i dimension:alue alue:_091_101_Vironniemen_peruspiiri . 
 ?i rdf:value ?val . FILTER ( datatype(?val) = xsd:double ) 
} 
ORDER BY ?ik ?v
</pre> 

<h3>Taulukko 2. Väkiluku äidinkielen mukaan 1. tammikuuta</h3>
<pre>
SELECT ?ika ?aidinkieli ?v ?val 
WHERE { 
 ?i dimension:vuosi ?v .  
 FILTER ( ?v = vuosi:_2000 || ?v = vuosi:_2008 || ?v = vuosi:_2009 || ?v = vuosi:_2010 ) 
 ?i dimension:yksikkö yksikkö:Henkilö . 
 ?i dimension:äidinkieli ?aidinkieli . 
 ?i dimension:ikä ?ika . 
 ?i dimension:sukupuoli sukupuoli:Molemmat_sukupuolet .
 ?i dimension:alue alue:_091_101_Vironniemen_peruspiiri . 
 ?i rdf:value ?val . 
 FILTER ( datatype(?val) = xsd:double ) 
} 
ORDER BY ?ika ?aidinkieli</pre>

<h3>Taulukko 3. Väestönmuutokset</h3>
<pre>
SELECT ?k ?v ?val 
WHERE { 
 ?i dimension:vuosi ?v .  
 FILTER ( ?v = vuosi:_2000 || ?v = vuosi:_2008 || ?v = vuosi:_2009 || ?v = vuosi:_2010 ) 
 ?i dimension:yksikkö yksikkö:Henkilö .
 ?i dimension:ikä ikä:Väestö_yhteensä . 
 ?i dimension:muuttosuunta ?k .  
 ?i dimension:alue alue:_091_101_Vironniemen_peruspiiri . 
 ?i rdf:value ?val . FILTER ( datatype(?val) = xsd:double ) 
} 
ORDER BY ?k ?v
</pre>

<h3>Taulukko 4. Asuntokanta hallintaperusteen ja huoneistotyypin mukaan 31.12.2009 (viimeinen vuosi)</h3>
<pre>
SELECT ?ha ?hu sum(?val) as ?asuntoja
WHERE { 
 ?i dimension:vuosi vuosi:_2008 .
 ?i dimension:yksikkö yksikkö:Asunto_ja_neliömetri . 
 ?i dimension:hallintaperuste ?ha2 . ?ha2 skos:broader ?ha . 
 ?i dimension:huoneistotyyppi ?hu2 . ?hu2 skos:broader ?hu . 
 ?i dimension:alue alue:_091_101_Vironniemen_peruspiiri . 
 ?i rdf:value ?val . FILTER ( datatype(?val) = xsd:double ) 
} 
GROUP BY ?ha ?hu
ORDER BY ?ha ?hu
</pre>

<h3>Taulukko 5. Rakennukset 31.12.2008 (viimeinen vuosi)</h3>
<pre>
SELECT ?kt ?yk ?val 
WHERE { 
 ?i dimension:vuosi vuosi:_2008 .
 ?i dimension:valmistumisvuosi valmistumisvuosi:Yhteensä . 
 ?i dimension:käyttötarkoitus_ja_kerrosluku ?kt . ?kt rdf:type dimension:Käyttötarkoitus_ja_kerrosluku . 
 ?i dimension:yksikkö ?yk . 
 ?i dimension:alue alue:_091_101_Vironniemen_peruspiiri . 
 ?i rdf:value ?val . FILTER ( datatype(?val) = xsd:double ) 
} 
ORDER BY ?kt ?yk
</pre>

<h3>Taulukko 6. Asuntotuotanto (kolme viimeistä vuotta)</h3>
<pre>
SELECT ?k ?v sum(?val) as ?asuntoja
WHERE { 
 GRAPH dataset:A01HKI_Astuot_hper_rahoitus_talotyyppi {
   ?i dimension:alue alue:_091_101_Vironniemen_peruspiiri . 
   ?i dimension:yksikkö yksikkö:Asuntojen_lukumäärä .
   ?i dimension:hallintaperuste hallintaperuste:Asunnot_yhteensä .
   ?i dimension:rahoitusmuoto rahoitusmuoto:Yhteensä .
   ?i dimension:talotyyppi talotyyppi:Yhteensä . 
   ?i rdf:value ?val . FILTER ( datatype(?val) = xsd:double ) 
   ?i dimension:vuosi ?v . 
   FILTER ( ?v = vuosi:_2007 || ?v = vuosi:_2008 || ?v = vuosi:_2009 ) 
   ?i dimension:huoneistotyyppi ?k2 .
 } 
 ?k2 skos:broader ?k . 
 ?v rdf:type dimension:Vuosi . 
} 
GROUP BY ?k ?v
ORDER BY ?k ?v
</pre>

<h3>Taulukko 7. Väestön keskitulo, euroa, vuonna 2008 (viimeinen vuosi)</h3>
<pre>
SELECT ?val 
WHERE { 
 ?i dimension:vuosi vuosi:_2008 .
 ?i dimension:tuloluokka tuloluokka:Keskitulo .
 ?i dimension:yksikkö yksikkö:Henkilö_ja_euro . 
 ?i dimension:alue alue:_091_101_Vironniemen_peruspiiri . 
 ?i rdf:value ?val . FILTER ( datatype(?val) = xsd:double ) 
} 
</pre>

<h3>Taulukko 7. Työpaikat toimialan mukaan (kolme viimeistä vuotta)</h3>
<pre>
SELECT ?t ?v sum(?val) as ?työpaikkoja
WHERE { 
 ?i dimension:yksikkö yksikkö:Henkilö . 
 ?i dimension:vuosi ?v .  
 FILTER ( ?v = vuosi:_2005 || ?v = vuosi:_2006 || ?v = vuosi:_2007 ) 
 ?i dimension:toimiala ?t2 . ?t2 skos:broader ?t . 
 ?i dimension:alue alue:_091_101_Vironniemen_peruspiiri . 
 ?i rdf:value ?val . FILTER ( datatype(?val) = xsd:double ) 
} 
GROUP BY ?t ?v
</pre>

</html>